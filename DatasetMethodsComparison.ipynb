{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DatasetMethodsComparison.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ivan-Nebogatikov/HumanActivityRecognition/blob/master/DatasetMethodsComparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiC378yxNzgY",
        "outputId": "da142246-ec74-4fcc-db27-08c73d020c62"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import json\r\n",
        "from datetime import datetime\r\n",
        "from datetime import date\r\n",
        "from math import sqrt\r\n",
        "\r\n",
        "def getValue(x):\r\n",
        "    l = list(json.loads(x.replace('\\'', '\"')))\r\n",
        "    return l\r\n",
        "\r\n",
        "def getDiff(x):\r\n",
        "    res = list()\r\n",
        "    for i, v in enumerate(x):\r\n",
        "        res.append(v)\r\n",
        "        #if i > 0:\r\n",
        "        #    res.append(v - x[i - 1])\r\n",
        "        #else:\r\n",
        "        #    res.append(0)\r\n",
        "    return res\r\n",
        "\r\n",
        "features = pd.read_csv('https://raw.githubusercontent.com/Ivan-Nebogatikov/HumanActivityRecognition/master/datasets/WinterDataset/dataset.csv', sep=r'\\s*,\\s*')\r\n",
        "features = features.drop('STATE', axis=1)\r\n",
        "print(features)\r\n",
        "activities = list(sorted(set(features['ACT'])))\r\n",
        "print(\"Activities:\", activities)\r\n",
        "\r\n",
        "labels = np.array(features['ACT'])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "        HR  SC  WF  BT        ACT\n",
            "0        0   0 -73 -54        eat\n",
            "1        0   0 -76 -39        eat\n",
            "2        0   0 -73 -27        eat\n",
            "3        0   0 -71 -27        eat\n",
            "4        0   0 -73 -27        eat\n",
            "...    ...  ..  ..  ..        ...\n",
            "26704  110   0 -53 -45  household\n",
            "26705  112   0 -53 -40  household\n",
            "26706  113   0 -53 -40  household\n",
            "26707  113   0 -53 -40  household\n",
            "26708  113   0 -53 -37  household\n",
            "\n",
            "[26709 rows x 5 columns]\n",
            "Activities: ['eat', 'household', 'inactive', 'preparing_food', 'shower', 'sport', 'videogames', 'walk', 'work']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0h32QTz0N2L0"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from scipy import interp\r\n",
        "\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "from sklearn.metrics import precision_recall_fscore_support\r\n",
        "from sklearn.metrics import roc_curve, auc\r\n",
        "from sklearn.preprocessing import LabelBinarizer\r\n",
        "\r\n",
        "def class_report(y_true, y_pred, y_score=None, average='micro'):\r\n",
        "    if y_true.shape != y_pred.shape:\r\n",
        "        print(\"Error! y_true %s is not the same shape as y_pred %s\" % (\r\n",
        "              y_true.shape,\r\n",
        "              y_pred.shape)\r\n",
        "        )\r\n",
        "        return\r\n",
        "    accuracy = accuracy_score(y_true, y_pred)\r\n",
        "    print(\"Accuracy:\", accuracy)\r\n",
        "\r\n",
        "    lb = LabelBinarizer()\r\n",
        "\r\n",
        "    if len(y_true.shape) == 1:\r\n",
        "        lb.fit(y_true)\r\n",
        "\r\n",
        "    #Value counts of predictions\r\n",
        "    labels, cnt = np.unique(\r\n",
        "        y_pred,\r\n",
        "        return_counts=True)\r\n",
        "    n_classes = 5\r\n",
        "    pred_cnt = pd.Series(cnt, index=labels)\r\n",
        "\r\n",
        "    metrics_summary = precision_recall_fscore_support(\r\n",
        "            y_true=y_true,\r\n",
        "            y_pred=y_pred,\r\n",
        "            labels=labels)\r\n",
        "\r\n",
        "    avg = list(precision_recall_fscore_support(\r\n",
        "            y_true=y_true, \r\n",
        "            y_pred=y_pred,\r\n",
        "            average='weighted'))\r\n",
        "\r\n",
        "    metrics_sum_index = ['precision', 'recall', 'f1-score', 'support']\r\n",
        "    class_report_df = pd.DataFrame(\r\n",
        "        list(metrics_summary),\r\n",
        "        index=metrics_sum_index,\r\n",
        "        columns=labels)\r\n",
        "\r\n",
        "    support = class_report_df.loc['support']\r\n",
        "    total = support.sum() \r\n",
        "    class_report_df['avg / total'] = avg[:-1] + [total]\r\n",
        "\r\n",
        "    class_report_df = class_report_df.T\r\n",
        "    class_report_df['pred'] = pred_cnt\r\n",
        "    class_report_df['pred'].iloc[-1] = total\r\n",
        "\r\n",
        "    if not (y_score is None):\r\n",
        "        fpr = dict()\r\n",
        "        tpr = dict()\r\n",
        "        roc_auc = dict()\r\n",
        "        for label_it, label in enumerate(labels):\r\n",
        "            fpr[label], tpr[label], _ = roc_curve(\r\n",
        "                (y_true == label).astype(int), \r\n",
        "                y_score[:, label_it])\r\n",
        "\r\n",
        "            roc_auc[label] = auc(fpr[label], tpr[label])\r\n",
        "\r\n",
        "        if average == 'micro':\r\n",
        "            if n_classes <= 2:\r\n",
        "                fpr[\"avg / total\"], tpr[\"avg / total\"], _ = roc_curve(\r\n",
        "                    lb.transform(y_true).ravel(), \r\n",
        "                    y_score[:, 1].ravel())\r\n",
        "            else:\r\n",
        "                fpr[\"avg / total\"], tpr[\"avg / total\"], _ = roc_curve(\r\n",
        "                        lb.transform(y_true).ravel(), \r\n",
        "                        y_score.ravel())\r\n",
        "\r\n",
        "            roc_auc[\"avg / total\"] = auc(\r\n",
        "                fpr[\"avg / total\"], \r\n",
        "                tpr[\"avg / total\"])\r\n",
        "\r\n",
        "        elif average == 'macro':\r\n",
        "            # First aggregate all false positive rates\r\n",
        "            all_fpr = np.unique(np.concatenate([\r\n",
        "                fpr[i] for i in labels]\r\n",
        "            ))\r\n",
        "\r\n",
        "            # Then interpolate all ROC curves at this points\r\n",
        "            mean_tpr = np.zeros_like(all_fpr)\r\n",
        "            for i in labels:\r\n",
        "                mean_tpr += interp(all_fpr, fpr[i], tpr[i])\r\n",
        "\r\n",
        "            # Finally average it and compute AUC\r\n",
        "            mean_tpr /= n_classes\r\n",
        "\r\n",
        "            fpr[\"macro\"] = all_fpr\r\n",
        "            tpr[\"macro\"] = mean_tpr\r\n",
        "\r\n",
        "            roc_auc[\"avg / total\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\r\n",
        "\r\n",
        "        class_report_df['AUC'] = pd.Series(roc_auc)\r\n",
        "\r\n",
        "    print(class_report_df)\r\n",
        "    return accuracy"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yX-W1FoPN48E"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "from sklearn.metrics import roc_auc_score\r\n",
        "from sklearn import metrics\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.neighbors import KNeighborsClassifier\r\n",
        "from sklearn.neural_network import MLPClassifier\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "def Predict(x, classifier = RandomForestClassifier(n_estimators = 400, random_state = 3, class_weight='balanced')):\r\n",
        "    train_features, test_features, train_labels, test_labels = train_test_split(x, labels, test_size = 0.15, random_state = 242)\r\n",
        "    print('Training Features Shape:', train_features.shape)\r\n",
        "    print('Testing Features Shape:', test_features.shape)\r\n",
        "    print(\"\\n\")\r\n",
        "\r\n",
        "    classifier.fit(train_features, train_labels);\r\n",
        "\r\n",
        "    predictions = list(classifier.predict(test_features))\r\n",
        "    pred_prob = classifier.predict_proba(test_features)\r\n",
        "\r\n",
        "    accuracy = class_report(\r\n",
        "        y_true=test_labels, \r\n",
        "        y_pred=np.asarray(predictions), \r\n",
        "        y_score=pred_prob, average='micro')\r\n",
        "    \r\n",
        "    if hasattr(classifier, 'feature_importances_'):\r\n",
        "      print(classifier.feature_importances_)\r\n",
        "\r\n",
        "    return accuracy\r\n",
        "\r\n",
        "def PredictWithClassifiers(data, classifiers):\r\n",
        "    accuracies = {}\r\n",
        "    for name, value in classifiers.items():\r\n",
        "      print(name + \"\\n\")\r\n",
        "      accuracy = Predict(data, value)\r\n",
        "      accuracies[name] = accuracy\r\n",
        "      print(\"\\n\")\r\n",
        "    df = pd.DataFrame(accuracies.items(), columns=[\"Method\", \"Accuracy\"])\r\n",
        "    print(df)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xt2zMUTeN7bD"
      },
      "source": [
        "from sklearn import svm\r\n",
        "from sklearn.naive_bayes import GaussianNB\r\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\r\n",
        "from sklearn.gaussian_process.kernels import RBF\r\n",
        "from sklearn.ensemble import AdaBoostClassifier\r\n",
        "\r\n",
        "methods = {\r\n",
        "    \"MLP\" : MLPClassifier(random_state=1, max_iter=300),\r\n",
        "    \"K-neigh\" : KNeighborsClassifier(), # default k = 5\r\n",
        "    \"Random Forest\" : RandomForestClassifier(n_estimators = 400, random_state = 3, class_weight='balanced'),\r\n",
        "    \"Bayes\" : GaussianNB(),\r\n",
        "    \"AdaBoost\" : AdaBoostClassifier(),\r\n",
        "    \"SVM\" : svm.SVC(probability=True, class_weight='balanced')\r\n",
        "}"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5wQSgD6N9uD",
        "outputId": "9b4a5f4d-55d1-4963-e936-797f4f42da6c"
      },
      "source": [
        "features2 = features.drop('ACT', axis = 1)\r\n",
        "\r\n",
        "PredictWithClassifiers(features2, methods)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLP\n",
            "\n",
            "Training Features Shape: (22702, 4)\n",
            "Testing Features Shape: (4007, 4)\n",
            "\n",
            "\n",
            "Accuracy: 0.7392063888195658\n",
            "                precision    recall  f1-score  support    pred       AUC\n",
            "eat              0.484375  0.436620  0.459259    213.0   192.0  0.916992\n",
            "household        0.907104  0.959538  0.932584    173.0   183.0  0.999326\n",
            "inactive         0.859310  0.916176  0.886833    680.0   725.0  0.984035\n",
            "preparing_food   0.719577  0.814371  0.764045    167.0   189.0  0.990510\n",
            "shower           0.905579  0.483945  0.630792    436.0   233.0  0.946058\n",
            "sport            0.482759  0.191781  0.274510     73.0    29.0  0.917220\n",
            "videogames       0.655374  0.794618  0.718310    706.0   856.0  0.945875\n",
            "walk             0.408602  0.305835  0.349827    497.0   372.0  0.819987\n",
            "work             0.819218  0.947269  0.878603   1062.0  1228.0  0.975372\n",
            "avg / total      0.731334  0.739206  0.723467   4007.0  4007.0  0.964548\n",
            "\n",
            "\n",
            "K-neigh\n",
            "\n",
            "Training Features Shape: (22702, 4)\n",
            "Testing Features Shape: (4007, 4)\n",
            "\n",
            "\n",
            "Accuracy: 0.8145744946343898\n",
            "                precision    recall  f1-score  support    pred       AUC\n",
            "eat              0.515385  0.629108  0.566596    213.0   260.0  0.930579\n",
            "household        1.000000  1.000000  1.000000    173.0   173.0  1.000000\n",
            "inactive         0.845517  0.901471  0.872598    680.0   725.0  0.971040\n",
            "preparing_food   1.000000  0.994012  0.996997    167.0   166.0  0.997005\n",
            "shower           0.823113  0.800459  0.811628    436.0   424.0  0.954741\n",
            "sport            0.578947  0.452055  0.507692     73.0    57.0  0.867420\n",
            "videogames       0.723958  0.787535  0.754410    706.0   768.0  0.939105\n",
            "walk             0.691877  0.496982  0.578454    497.0   357.0  0.882026\n",
            "work             0.922006  0.935028  0.928471   1062.0  1077.0  0.984037\n",
            "avg / total      0.813580  0.814574  0.811236   4007.0  4007.0  0.962201\n",
            "\n",
            "\n",
            "Random Forest\n",
            "\n",
            "Training Features Shape: (22702, 4)\n",
            "Testing Features Shape: (4007, 4)\n",
            "\n",
            "\n",
            "Accuracy: 0.8243074619416022\n",
            "                precision    recall  f1-score  support    pred       AUC\n",
            "eat              0.487879  0.755869  0.593002    213.0   330.0  0.964354\n",
            "household        1.000000  1.000000  1.000000    173.0   173.0  1.000000\n",
            "inactive         0.877292  0.914706  0.895608    680.0   709.0  0.988319\n",
            "preparing_food   1.000000  1.000000  1.000000    167.0   167.0  1.000000\n",
            "shower           0.871795  0.779817  0.823245    436.0   390.0  0.986023\n",
            "sport            0.304094  0.712329  0.426230     73.0   171.0  0.974481\n",
            "videogames       0.788018  0.726629  0.756080    706.0   651.0  0.964942\n",
            "walk             0.767760  0.565392  0.651217    497.0   366.0  0.926623\n",
            "work             0.946667  0.935970  0.941288   1062.0  1050.0  0.994015\n",
            "avg / total      0.845035  0.824307  0.829166   4007.0  4007.0  0.985095\n",
            "[0.34571379 0.         0.4850618  0.16922442]\n",
            "\n",
            "\n",
            "Bayes\n",
            "\n",
            "Training Features Shape: (22702, 4)\n",
            "Testing Features Shape: (4007, 4)\n",
            "\n",
            "\n",
            "Accuracy: 0.6765660094834041\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                precision    recall  f1-score  support    pred       AUC\n",
            "household        0.699187  0.994220  0.821002    173.0   246.0  0.001755\n",
            "inactive         0.751816  0.913235  0.824701    680.0   826.0  0.154732\n",
            "preparing_food   1.000000  0.940120  0.969136    167.0   157.0  0.218750\n",
            "shower           0.976744  0.481651  0.645161    436.0   215.0  0.719873\n",
            "sport            0.358974  0.383562  0.370861     73.0    78.0  0.735802\n",
            "videogames       0.425787  0.804533  0.556863    706.0  1334.0  0.672858\n",
            "walk             0.474576  0.169014  0.249258    497.0   177.0  0.615275\n",
            "work             0.894251  0.820151  0.855599   1062.0   974.0  0.743147\n",
            "avg / total      0.683160  0.676566  0.648543   3794.0  3794.0  0.909899\n",
            "\n",
            "\n",
            "AdaBoost\n",
            "\n",
            "Training Features Shape: (22702, 4)\n",
            "Testing Features Shape: (4007, 4)\n",
            "\n",
            "\n",
            "Accuracy: 0.4564512103818318\n",
            "             precision    recall  f1-score  support    pred       AUC\n",
            "household     1.000000  1.000000  1.000000    173.0   173.0  0.005999\n",
            "shower        1.000000  0.052752  0.100218    436.0    23.0  0.366744\n",
            "videogames    0.254690  1.000000  0.405980    706.0  2772.0  0.687065\n",
            "work          0.892204  0.872881  0.882437   1062.0  1039.0  0.921655\n",
            "avg / total   0.433325  0.456451  0.359487   2377.0  2377.0  0.785875\n",
            "[0.5 0.  0.5 0. ]\n",
            "\n",
            "\n",
            "SVM\n",
            "\n",
            "Training Features Shape: (22702, 4)\n",
            "Testing Features Shape: (4007, 4)\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6790616421262791\n",
            "                precision    recall  f1-score  support    pred       AUC\n",
            "eat              0.284936  0.737089  0.410995    213.0   551.0  0.886583\n",
            "household        0.742489  1.000000  0.852217    173.0   233.0  0.999934\n",
            "inactive         0.747031  0.925000  0.826544    680.0   842.0  0.951940\n",
            "preparing_food   0.491749  0.892216  0.634043    167.0   303.0  0.974344\n",
            "shower           0.875502  0.500000  0.636496    436.0   249.0  0.932407\n",
            "sport            0.357895  0.465753  0.404762     73.0    95.0  0.947263\n",
            "videogames       0.592593  0.657224  0.623237    706.0   783.0  0.890176\n",
            "walk             0.777778  0.014085  0.027668    497.0     9.0  0.773071\n",
            "work             0.944798  0.838041  0.888224   1062.0   942.0  0.960852\n",
            "avg / total      0.747540  0.679062  0.650616   4007.0  4007.0  0.948099\n",
            "\n",
            "\n",
            "          Method  Accuracy\n",
            "0            MLP  0.739206\n",
            "1        K-neigh  0.814574\n",
            "2  Random Forest  0.824307\n",
            "3          Bayes  0.676566\n",
            "4       AdaBoost  0.456451\n",
            "5            SVM  0.679062\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}